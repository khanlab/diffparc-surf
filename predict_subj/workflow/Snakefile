import os
from glob import glob

configfile: 'config/config.yml'

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider()

envvars:
    'AWS_ACCESS_KEY_ID',
    'AWS_SECRET_ACCESS_KEY'

rule all:
    input:
        config['out_csv']

rule unzip_model:
    input:
        S3.remote(config['s3_model_zip'])
    output:
        model_dir=directory('model')
    shell:
        'mkdir -p {output.model_dir} && unzip -d {output} {input}'


rule extract_archive:
    input:
        archive=config['in_archive']
    output:
        dicom_dir=temp(directory('dicoms'))
    script: 'scripts/extract_archive.py'

#zip of dicoms = extract and flatten, next step is to_nifti 
#zip of bids - extract as is, but will need to know what subject/session to process

checkpoint to_nifti:
    input:
        dicom_dir='dicoms'
    output:
        nifti_dir=temp(directory('niftis'))
    container:
        config['singularity']['dcm2niix']
    shell:
        'mkdir -p {output} && dcm2niix -i y -o {output}  {input} '

   
rule cp_t1w_files:
    """this rule uses a list of possible matches to find the T1w nii"""
    input:
        nifti_dir='niftis'
    output: 
        t1='bids/sub-01/anat/sub-01_T1w.nii'
    script:
        'scripts/cp_t1w_file.py'
       
rule gzip_t1:
    input:
        t1='bids/sub-01/anat/sub-01_T1w.nii'
    output:
        t1='bids/sub-01/anat/sub-01_T1w.nii.gz'
    shell:
        'gzip {input}'
 
rule cp_dwi:
    input:
        nifti_dir='niftis'
    output:
        dwi_dir=directory('bids/sub-01/dwi')
    shell:
        "mkdir -p {output.dwi_dir} && i=1; "
        "for bvec in `ls {input.nifti_dir}/*.bvec`; "
        "do "
        "  prefix=${{bvec%.bvec}}; "
        "  cp -v ${{prefix}}.nii {output.dwi_dir}/sub-01_run-${{i}}_dwi.nii; "
        "  gzip {output.dwi_dir}/sub-01_run-${{i}}_dwi.nii; "
        "  cp -v ${{prefix}}.bval {output.dwi_dir}/sub-01_run-${{i}}_dwi.bval; "
        "  cp -v ${{prefix}}.bvec {output.dwi_dir}/sub-01_run-${{i}}_dwi.bvec; "
        "  cp -v ${{prefix}}.json {output.dwi_dir}/sub-01_run-${{i}}_dwi.json; "
        "  i=$((i+1)); "
        "done "
        

rule cp_dd:
    input:
        'resources/dataset_description.json'
    output:
        'bids/dataset_description.json'
    shell:
        'cp {input} {output}'

rule run_diffparc:
    input:
        t1='bids/sub-01/anat/sub-01_T1w.nii.gz',
        dwi='bids/sub-01/dwi',
        dd='bids/dataset_description.json',
        diffparc_wf='diffparc-wf'
    container: config['singularity']['diffparc_deps']
    output:
        directory('diffparc')
    shadow: 'minimal'
    threads: 32
    shell: 
        'SNAKEMAKE_PROFILE="" {input.diffparc_wf}/run.py bids diffparc participant --use-template-parcellation -c {threads} '



# concat all subj/sess for a given metric/seed/target combination (wildcards metric and seed; lookup targets using seed), get subjects with simple glob()
rule concat_subj_csv:
    input:
        extract_dir="diffparc"
    params:
        feature_csv_globs=lambda wildcards: config["feature_csv_globs"][
            wildcards.kind
        ],
    output:
        csv="tabular/raw_features/sub-01_kind-{kind}_desc-raw_feats.csv",  # wide-form csv has column for each feature+roi
    container: config['singularity']['tabular_deps']
    script:
        "scripts/concat_globbed_csvs.py"


rule rename_feature_columns:
    input:
        csv="tabular/raw_features/sub-01_kind-{kind}_desc-raw_feats.csv",
    output:
        csv="tabular/renamed_features/sub-01_kind-{kind}_desc-raw_feats.csv",
    container: config['singularity']['tabular_deps']
    script:
        "scripts/rename_feature_columns.py"


rule merge_features:
    input:
        csvs=lambda wildcards: expand(
            "tabular/renamed_features/sub-01_kind-{kind}_desc-raw_feats.csv",
            kind=config["feature_sets"][wildcards.feature_set],
            allow_missing=True,
        ),
    params:
        extract_ids=lambda wildcards: config["tabular"][
            "extract_ids"
        ],
        bad_subjects=[]
    output:
        csv="tabular/wide_features/{feature_set}/sub-01_desc-raw_feats.csv",
    container: config['singularity']['tabular_deps']
    script:
        "scripts/merge_features.py"


rule get_all_feature_names:
    input:
        csv="tabular/wide_features/{feature_set}/sub-01_desc-raw_feats.csv",
    output:
        features=directory("tabular/feature_names/{feature_set}"),
    container: config['singularity']['tabular_deps']
    script: 'scripts/get_all_feature_names.py'

rule feature_select:
    """just creates csv with all features"""
    input:
        features="tabular/feature_names/{feature_set}",
    output:
        selected_features="tabular/feature_names/{feature_set}.csv"
    container: config['singularity']['tabular_deps']
    script:
        "scripts/feature_select.py"



rule add_metadata_to_csv:
    input:
        csv="tabular/wide_features/{feature_set}/sub-01_desc-raw_feats.csv",
    params:
        metadata={'age': config['age'], 'sex': config['sex']}
    output:
        csv="tabular/demog_features/{feature_set}/sub-01_desc-demog_feats.csv",
    container: config['singularity']['tabular_deps']
    script:
        "scripts/add_subj_metadata_to_csv.py"

rule predict_photonai:
    input:
        project_folder="model",
        test_csv="tabular/demog_features/{feature_set}/sub-01_desc-demog_feats.csv",
        features="tabular/feature_names/{feature_set}.csv"
    output:
        predictions_csv="tabular/sub-01_featset-{feature_set}_predictions.csv",
    container: config['singularity']['tabular_deps']
    script:
        "scripts/predict_photonai.py"

rule copy_final:
    input: "tabular/sub-01_featset-{feature_set}_predictions.csv".format(feature_set=config['feature_set'])
    output: config['out_csv']
    shell: 'cp {input} {output}'
